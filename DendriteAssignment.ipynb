{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8676388,"sourceType":"datasetVersion","datasetId":5200741},{"sourceId":8676406,"sourceType":"datasetVersion","datasetId":5200755}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-17T13:35:51.611496Z","iopub.execute_input":"2024-06-17T13:35:51.611995Z","iopub.status.idle":"2024-06-17T13:35:51.626776Z","shell.execute_reply.started":"2024-06-17T13:35:51.611958Z","shell.execute_reply":"2024-06-17T13:35:51.625590Z"},"trusted":true},"execution_count":83,"outputs":[{"name":"stdout","text":"/kaggle/input/iris-dataset-dendrite/iris.csv\n/kaggle/input/jason-parsing/algoparams_from_ui.json.rtf\n","output_type":"stream"}]},{"cell_type":"code","source":"import json\nimport csv\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor, ExtraTreesClassifier, ExtraTreesRegressor\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso, ElasticNet, SGDClassifier, SGDRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nimport re\nimport string\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:35:51.704005Z","iopub.execute_input":"2024-06-17T13:35:51.704931Z","iopub.status.idle":"2024-06-17T13:35:51.713217Z","shell.execute_reply.started":"2024-06-17T13:35:51.704896Z","shell.execute_reply":"2024-06-17T13:35:51.712045Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"!pip install pypandoc","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:35:51.715375Z","iopub.execute_input":"2024-06-17T13:35:51.715808Z","iopub.status.idle":"2024-06-17T13:36:06.509697Z","shell.execute_reply.started":"2024-06-17T13:35:51.715768Z","shell.execute_reply":"2024-06-17T13:36:06.508330Z"},"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pypandoc in /opt/conda/lib/python3.10/site-packages (1.13)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pypandoc\n\ndef convert_rtf_to_text(file_path):\n    output = pypandoc.convert_file(json_file_path, 'plain', format='rtf')\n    return output\n\n\njson_file_path = '/kaggle/input/jason-parsing/algoparams_from_ui.json.rtf'\n# json_file_path = 'algoparams_from_ui.json.rtf'\ntext = convert_rtf_to_text(json_file_path)\n\n\n# Stripping newline and spaces to create a dictionary\nstripped_text = text.replace('\\n', '')\ntext = stripped_text.replace(' ', '')\nconstraint_dict = json.loads(text)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:36:06.512558Z","iopub.execute_input":"2024-06-17T13:36:06.512986Z","iopub.status.idle":"2024-06-17T13:36:06.654008Z","shell.execute_reply.started":"2024-06-17T13:36:06.512949Z","shell.execute_reply":"2024-06-17T13:36:06.652805Z"},"trusted":true},"execution_count":86,"outputs":[]},{"cell_type":"code","source":"def feature_handling(X):\n    # Removing Features based on the given input\n    if(constraint_dict['design_state_data']['feature_handling']['sepal_length']['is_selected'] == False):\n        print(\"sepal_length False\")\n        X = X.drop('sepal_length', axis = 1)\n\n    if(constraint_dict['design_state_data']['feature_handling']['sepal_width']['is_selected'] == False):\n        print(\"False\")\n        X = X.drop('sepal_width', axis = 1)\n\n    if(constraint_dict['design_state_data']['feature_handling']['petal_length']['is_selected'] == False):\n        print(\"False\")\n        X = X.drop('petal_length', axis = 1)\n\n    if(constraint_dict['design_state_data']['feature_handling']['petal_width']['is_selected'] == False):\n        print(\"False\")\n        X = X.drop('petal_width', axis = 1)\n\n#     Impute the data as specified in the input values\n    if(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_with'] == 'Averageofvalues'):\n        imputer = SimpleImputer(strategy=\"mean\")\n        X['sepal_length'] = imputer.fit_transform(X[['sepal_length']])\n    elif(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_with'] == 'custom'):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_value']))\n        X['sepal_length'] = imputer.fit_transform(X[['sepal_length']])\n\n        \n    if(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_with'] == 'Averageofvalues'):\n        imputer = SimpleImputer(strategy=\"mean\")\n        X['sepal_width'] = imputer.fit_transform(X[['sepal_width']])\n    elif(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_with'] == 'custom'):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_value']))\n        X['sepal_width'] = imputer.fit_transform(X[['sepal_width']])\n\n\n    if(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_with'] == 'Averageofvalues'):\n        imputer = SimpleImputer(strategy=\"mean\")\n        X['petal_length'] = imputer.fit_transform(X[['petal_length']])\n    elif(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_with'] == 'custom'):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_value']))\n        X['petal_length'] = imputer.fit_transform(X[['petal_length']])\n\n\n    if(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_with'] == 'Averageofvalues'):\n        imputer = SimpleImputer(strategy=\"mean\")\n        X['petal_width'] = imputer.fit_transform(X[['petal_width']])\n    elif(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_with'] == 'custom'):\n        imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_value']))\n        X['petal_width'] = imputer.fit_transform(X[['petal_width']])\n    return X","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:36:06.655706Z","iopub.execute_input":"2024-06-17T13:36:06.656083Z","iopub.status.idle":"2024-06-17T13:36:06.678016Z","shell.execute_reply.started":"2024-06-17T13:36:06.656052Z","shell.execute_reply":"2024-06-17T13:36:06.676683Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# Set the algorithm parameters such that it runs as specified in the input\n\n# Classification parameters for algorithm tuning\nsvc_kernels = {'linear_kernel': 'linear', 'rep_kernel': 'rbf', 'polynomial_kernel': 'poly', 'sigmoid_kernel': 'sigmoid'}\n\nsvc_regs = {'use_l1_regularization': 'l1', 'use_l2_regularization': 'l2', 'use_elastic_net_regularization': 'elasticnet'}\n\ndc_impure = {'use_gini' : 'gini', 'use_entropy' : 'entropy'}\n\ndc_splitter = {'use_best' : 'best', 'use_random' : 'random'}\n\nsvr_kernels = {'linear_kernel': 'linear', 'rep_kernel': 'rbf', 'polynomial_kernel': 'poly', 'sigmoid_kernel': 'sigmoid'}\n\nclf_rf_n_est = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['min_trees'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['max_trees']+5, 5))\n\nclf_rf_max_depth = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['min_depth'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['max_depth']+5,5))\n\nclf_rf_samp_leaf = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['min_samples_per_leaf_min_value'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestClassifier']['min_samples_per_leaf_max_value']+5,5))\n\nclf_gb_n_est = list((constraint_dict['design_state_data']['algorithms']['GBTClassifier']['num_of_BoostingStages'][0],\n                    constraint_dict['design_state_data']['algorithms']['GBTClassifier']['num_of_BoostingStages'][1]))\n\nclf_gb_minmax_subsamp = list((constraint_dict['design_state_data']['algorithms']['GBTClassifier']['min_subsample'],\n                          constraint_dict['design_state_data']['algorithms']['GBTClassifier']['max_subsample']))\n\nclf_gb_minmax_depth = list(range(constraint_dict['design_state_data']['algorithms']['GBTClassifier']['min_depth'],\n                          constraint_dict['design_state_data']['algorithms']['GBTClassifier']['max_depth']+1))\n\nclf_lr_n_jobs = constraint_dict['design_state_data']['algorithms']['LogisticRegression']['parallelism']\n\nclf_lr_max_iter = list(range(constraint_dict['design_state_data']['algorithms']['LogisticRegression']['min_iter'], \n                             constraint_dict['design_state_data']['algorithms']['LogisticRegression']['max_iter']+10, 10))\n\nclf_svc_list = []\nfor i in svc_kernels:\n    if(constraint_dict['design_state_data']['algorithms']['SVM'][i] == True):\n        clf_svc_list.append(svc_kernels[i])\n\nclf_svc_max_iter = constraint_dict['design_state_data']['algorithms']['SVM']['max_iterations']\n\nclf_svc_l_list = []\nfor i in svc_regs:\n    if(constraint_dict['design_state_data']['algorithms']['SGD'][i] == 'on' or\n      constraint_dict['design_state_data']['algorithms']['SGD'][i] == True):\n        clf_svc_l_list.append(svc_regs[i])\n\nclf_dc_depth = list(range(constraint_dict['design_state_data']['algorithms']['DecisionTreeClassifier']['min_depth'], constraint_dict['design_state_data']['algorithms']['DecisionTreeClassifier']['max_depth']+1))\n\nclf_dc_impure = []\nfor i in dc_impure:\n    if(constraint_dict['design_state_data']['algorithms']['DecisionTreeClassifier'][i] == True):\n        clf_dc_impure.append(dc_impure[i])\n\nclf_dc_split = []\nfor i in dc_splitter:\n    if(constraint_dict['design_state_data']['algorithms']['DecisionTreeClassifier'][i] == True):\n        clf_dc_split.append(dc_splitter[i])\n        \nclf_etree_parll = constraint_dict['design_state_data']['algorithms']['extra_random_trees']['parallelism']\n\nclf_nn_opt = constraint_dict['design_state_data']['algorithms']['neural_network']['solver']\nclf_nn_alpha = constraint_dict['design_state_data']['algorithms']['neural_network']['alpha_value']\nclf_nn_estop = constraint_dict['design_state_data']['algorithms']['neural_network']['early_stopping']\n\n\n# Regression parameters for algorithm tuning\n\nreg_rf_n_est = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['min_trees'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['max_trees']+5, 5))\n\nreg_rf_max_depth = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['min_depth'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['max_depth']+5, 5))\n\nreg_rf_samp_leaf = list(range(constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['min_samples_per_leaf_min_value'], \n                 constraint_dict['design_state_data']['algorithms']['RandomForestRegressor']['min_samples_per_leaf_max_value']+5,5))\n\nreg_gb_n_est = list((constraint_dict['design_state_data']['algorithms']['GBTRegressor']['num_of_BoostingStages'][0],\n                    constraint_dict['design_state_data']['algorithms']['GBTRegressor']['num_of_BoostingStages'][1]))\n\nreg_gb_minmax_subsamp = list((constraint_dict['design_state_data']['algorithms']['GBTRegressor']['min_subsample'],\n                          constraint_dict['design_state_data']['algorithms']['GBTRegressor']['max_subsample']))\n\nreg_gb_minmax_depth = list(range(constraint_dict['design_state_data']['algorithms']['GBTRegressor']['min_depth'],\n                          constraint_dict['design_state_data']['algorithms']['GBTRegressor']['max_depth']+1))\n\nreg_lreg_njobs = constraint_dict['design_state_data']['algorithms']['LinearRegression']['parallelism']\n\nreg_svr_list = []\nfor i in svr_kernels:\n    if(constraint_dict['design_state_data']['algorithms']['SVM'][i] == True):\n        reg_svr_list.append(svr_kernels[i])\n        \nreg_svr_max_iter = constraint_dict['design_state_data']['algorithms']['SVM']['max_iterations']","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:36:06.681412Z","iopub.execute_input":"2024-06-17T13:36:06.681810Z","iopub.status.idle":"2024-06-17T13:36:06.713554Z","shell.execute_reply.started":"2024-06-17T13:36:06.681777Z","shell.execute_reply":"2024-06-17T13:36:06.712292Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"# Define a list of models with parameter grids for classification\nclassification_models = [\n    ('RandomForestClassifier', RandomForestClassifier(), {\n        'classifier__n_estimators': clf_rf_n_est,\n        'classifier__max_depth': clf_rf_max_depth,\n        'classifier__min_samples_leaf': clf_rf_samp_leaf\n    }),\n    ('GradientBoostingClassifier', GradientBoostingClassifier(), {\n        'classifier__n_estimators': clf_gb_n_est,\n        'classifier__learning_rate': [0.01, 0.1, 1],\n        'classifier__max_depth': clf_gb_minmax_depth\n    }),\n    ('LogisticRegression', LogisticRegression(max_iter=1000), {\n        'classifier__C': [0.1, 1, 10],\n        'classifier__solver': ['liblinear', 'lbfgs'],\n        'classifier__n_jobs': clf_lr_n_jobs,\n        'classifier__max_iter' : clf_lr_max_iter\n    }),\n    ('SVC', SVC(), {\n        'classifier__kernel': clf_svc_list,\n        'classifier__C': [0.1, 1, 10],\n        'classifier__max_iter': clf_svc_max_iter\n    }),\n    ('SGDClassifier', SGDClassifier(max_iter=1000, tol=1e-3), {\n        'classifier__loss': ['hinge', 'log'],\n        'classifier__alpha': [0.0001, 0.001, 0.01],\n        'classifier__penalty' : clf_svc_l_list\n    }),\n    ('KNeighborsClassifier', KNeighborsClassifier(), {\n        'classifier__n_neighbors': [3, 5, 7],\n        'classifier__weights': ['uniform', 'distance'],\n        'classifier__algorithm' : ['auto', 'ball_tree', 'kd_tree', 'brute']\n    }),\n    ('DecisionTreeClassifier', DecisionTreeClassifier(), {\n        'classifier__max_depth': clf_dc_depth,\n        'classifier__criterion' : clf_dc_impure,\n        'classifier__splitter' : clf_dc_split\n    }),\n    ('ExtraTreesClassifier', ExtraTreesClassifier(), {\n        'classifier__n_estimators': [50, 100],\n        'classifier__max_depth': [None, 10, 20],\n        'classifier__n_jobs' : clf_etree_parll\n    }),\n    ('XGBClassifier', XGBClassifier(), {\n        'classifier__n_estimators': [50, 100],\n        'classifier__learning_rate': [0.01, 0.1, 1],\n        'classifier__max_depth': [3, 5]\n    }),\n    ('MLPClassifier', MLPClassifier(max_iter=1000), {\n        'classifier__hidden_layer_sizes': [(50,), (100,)],\n        'classifier__activation': ['tanh', 'relu'],\n        'classifier__alpha': clf_nn_alpha,\n        'classifier__solver' : clf_nn_opt.lower(),\n        'classifier__early_stopping' : clf_nn_estop\n    })\n]\n\n# Define a list of models with parameter grids for regression\nregression_models = [\n    ('RandomForestRegressor', RandomForestRegressor(), {\n        'regressor__n_estimators': reg_rf_n_est,\n        'regressor__max_depth': reg_rf_max_depth,\n        'regressor__min_samples_leaf': reg_rf_samp_leaf\n    }),\n    ('GradientBoostingRegressor', GradientBoostingRegressor(), {\n        'classifier__n_estimators': clf_gb_n_est,\n        'classifier__learning_rate': [0.01, 0.1, 1],\n        'classifier__max_depth': clf_gb_minmax_depth\n    }),\n    ('LinearRegression', LinearRegression(), {\n        'classifier__n_jobs' : reg_lreg_njobs\n    }),\n    ('Ridge', Ridge(), {\n        'regressor__alpha': [0.1, 1, 10]\n    }),\n    ('Lasso', Lasso(), {\n        'regressor__alpha': [0.1, 1, 10]\n    }),\n    ('ElasticNet', ElasticNet(), {\n        'regressor__alpha': [0.1, 1, 10],\n        'regressor__l1_ratio': [0.2, 0.5, 0.8]\n    }),\n    ('SVR', SVR(), {\n        'regressor__kernel': reg_svr_list,\n        'regressor__C': [0.1, 1, 10],\n        'regressor__max_iter': reg_svr_max_iter\n    }),\n    ('SGDRegressor', SGDRegressor(max_iter=1000, tol=1e-3), {\n        'regressor__loss': ['squared_loss', 'huber'],\n        'regressor__alpha': [0.0001, 0.001, 0.01],\n        'regressor__penalty': clf_svc_l_list\n    }),\n    ('KNeighborsRegressor', KNeighborsRegressor(), {\n        'regressor__n_neighbors': [3, 5, 7],\n        'regressor__weights': ['uniform', 'distance']\n    }),\n    ('DecisionTreeRegressor', DecisionTreeRegressor(), {\n        'regressor__max_depth': [None, 10, 20],\n        'regressor__min_samples_split': [2, 5]\n    }),\n    ('ExtraTreesRegressor', ExtraTreesRegressor(), {\n        'regressor__n_estimators': [50, 100],\n        'regressor__max_depth': [None, 10, 20]\n    }),\n    ('XGBRegressor', XGBRegressor(), {\n        'regressor__n_estimators': [50, 100],\n        'regressor__learning_rate': [0.01, 0.1, 1],\n        'regressor__max_depth': [3, 5]\n    }),\n    ('MLPRegressor', MLPRegressor(max_iter=1000), {\n        'regressor__hidden_layer_sizes': [(50,), (100,)],\n        'regressor__activation': ['tanh', 'relu'],\n        'regressor__alpha': [0.0001, 0.001]\n    })\n]\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:36:06.715529Z","iopub.execute_input":"2024-06-17T13:36:06.715906Z","iopub.status.idle":"2024-06-17T13:36:06.741921Z","shell.execute_reply.started":"2024-06-17T13:36:06.715874Z","shell.execute_reply":"2024-06-17T13:36:06.740612Z"},"trusted":true},"execution_count":89,"outputs":[]},{"cell_type":"code","source":"\"\"\" Loading the dataset \"\"\" \ndata = pd.read_csv('/kaggle/input/iris-dataset-dendrite/iris.csv')\n# data = pd.read_csv('iris-dataset-dendrite/iris.csv')\n\n\n# Split the dataset into features and target variable\nX = data.drop(columns='species') \ny = data['species']\n\n# Encoding the target\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\n\nX = feature_handling(X)\n\n# Setting grid parameters\nshuffle = constraint_dict['design_state_data']['hyperparameters']['shuffle_grid']\nrandom_state = constraint_dict['design_state_data']['hyperparameters']['random_state']\nsplits = constraint_dict['design_state_data']['hyperparameters']['num_of_folds']\nnjobs = constraint_dict['design_state_data']['hyperparameters']['parallelism']\n\n# Mapping between the input classification algorithm names to the available library function names\nclass_dict = {'RandomForestClassifier' : 'RandomForestClassifier',\n              'GradientBoostingClassifier' : 'GBTClassifier', \n              'LogisticRegression' : 'LogisticRegression',\n              'SVC' : 'SVM',\n              'SGDClassifier' : 'SGD',\n              'KNeighborsClassifier' : 'KNN',\n              'DecisionTreeClassifier' : 'DecisionTreeClassifier',\n              'ExtraTreesClassifier' : 'extra_random_trees', \n              'XGBClassifier' : 'xg_boost', \n              'MLPClassifier' : 'neural_network'}\n\n\n# Cross-validation strategies\ncv_classification = StratifiedKFold(n_splits=splits, shuffle=shuffle, random_state=random_state)\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Function to perform grid search for classification models\ndef perform_classification_grid_search(X_train, y_train, models):\n    print(\"*******Classification*********\")\n    results = []\n    for name, model, params in models:\n        model_name = re.sub(r\"[^\\w]\", \"\", name)\n#         print(constraint_dict['design_state_data']['algorithms'][class_dict[model_name]]['is_selected'], name)\n        if(constraint_dict['design_state_data']['algorithms'][class_dict[model_name]]['is_selected'] == True):\n            print(f\"Running GridSearchCV for {name}\")\n            pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                ('classifier', model)\n            ])\n            grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv_classification, scoring='accuracy', n_jobs=njobs, verbose=2)\n            grid_search.fit(X_train, y_train)\n            results.append((name, grid_search.best_params_, grid_search.best_score_))\n    return results\n\n\n# Perform grid search for classification models\nclassification_results = perform_classification_grid_search(X_train, y_train, classification_models)\n\n# Print the results for classification\nprint(\"Classification Results:\")\nfor result in classification_results:\n    print(f\"Model: {result[0]}\")\n    print(f\"Best Parameters: {result[1]}\")\n    print(f\"Best Cross-Validation Accuracy: {result[2]:.2f}\")\n    print(\"-\" * 30)\n\n# Mapping between the input regression algorithm names to the available library function names\nreg_dict = {'RandomForestRegressor':'RandomForestRegressor', \n            'GradientBoostingRegressor' : 'GBTRegressor', \n            'LinearRegression' : 'LinearRegression',\n            'Ridge': 'RidgeRegression', \n            'Lasso' : 'LassoRegression', \n            'ElasticNet' : 'ElasticNetRegression', \n            'SVR' : 'SVM', \n            'SGDRegressor' : 'SGD', \n            'KNeighborsRegressor' : 'KNN', \n            'DecisionTreeRegressor' : 'DecisionTreeRegressor', \n            'ExtraTreesRegressor' : 'extra_random_trees',\n            'XGBRegressor' : 'xg_boost', \n            'MLPRegressor' : 'neural_network'}\n\n# Function to perform grid search for regression models\ndef perform_regression_grid_search(X_train, y_train, models):\n    print(\"********Regression*********\")\n    results = []\n    for name, model, params in models:\n        model_name = re.sub(r\"[^\\w]\", \"\", name)\n        if(constraint_dict['design_state_data']['algorithms'][reg_dict[model_name]]['is_selected'] == True):\n            print(f\"Running GridSearchCV for {name}\")\n            pipeline = Pipeline([\n                ('scaler', StandardScaler()),\n                ('regressor', model)\n            ])\n            grid_search = GridSearchCV(pipeline, param_grid=params, cv=cv_regression, scoring='neg_mean_squared_error', n_jobs=njobs, verbose=2)\n            grid_search.fit(X_train, y_train)\n            results.append((name, grid_search.best_params_, grid_search.best_score_))\n    return results\n\n# Load regression data from CSV and split\ndata = pd.read_csv('/kaggle/input/iris-dataset-dendrite/iris.csv')    \nX = data.drop(columns = 'species')\nX = X.drop(columns= constraint_dict['design_state_data']['target']['target'])\ny = data[constraint_dict['design_state_data']['target']['target']]\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\ncv_regression = KFold(n_splits=splits, shuffle=shuffle, random_state=random_state)\n\n# Perform grid search for regression models\nregression_results = perform_regression_grid_search(X_train, y_train, regression_models)\n\n\n\n# Print the results for regression\nprint(\"Regression Results:\")\nfor result in regression_results:\n    print(f\"Model: {result[0]}\")\n    print(f\"Best Parameters: {result[1]}\")\n    print(f\"Best Cross-Validation MSE: {result[2]:.2f}\")\n    print(\"-\" * 30)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T13:36:45.031217Z","iopub.execute_input":"2024-06-17T13:36:45.031670Z","iopub.status.idle":"2024-06-17T13:36:46.164185Z","shell.execute_reply.started":"2024-06-17T13:36:45.031635Z","shell.execute_reply":"2024-06-17T13:36:46.163055Z"},"trusted":true},"execution_count":91,"outputs":[{"name":"stdout","text":"*******Classification*********\nClassification Results:\n********Regression*********\nRunning GridSearchCV for RandomForestRegressor\nFitting 6 folds for each of 12 candidates, totalling 72 fits\nRegression Results:\nModel: RandomForestRegressor\nBest Parameters: {'regressor__max_depth': 25, 'regressor__min_samples_leaf': 5, 'regressor__n_estimators': 15}\nBest Cross-Validation MSE: -0.03\n------------------------------\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}