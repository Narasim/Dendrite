{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8676388,"sourceType":"datasetVersion","datasetId":5200741},{"sourceId":8676406,"sourceType":"datasetVersion","datasetId":5200755}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport csv\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2024-06-14T17:51:38.162287Z","iopub.execute_input":"2024-06-14T17:51:38.162656Z","iopub.status.idle":"2024-06-14T17:51:39.428516Z","shell.execute_reply.started":"2024-06-14T17:51:38.162627Z","shell.execute_reply":"2024-06-14T17:51:39.427471Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install pypandoc","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:18:14.137805Z","iopub.execute_input":"2024-06-14T09:18:14.138220Z","iopub.status.idle":"2024-06-14T09:18:30.798385Z","shell.execute_reply.started":"2024-06-14T09:18:14.138188Z","shell.execute_reply":"2024-06-14T09:18:30.796839Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stdout","text":"Collecting pypandoc\n  Downloading pypandoc-1.13-py3-none-any.whl.metadata (16 kB)\nDownloading pypandoc-1.13-py3-none-any.whl (21 kB)\nInstalling collected packages: pypandoc\nSuccessfully installed pypandoc-1.13\n","output_type":"stream"}]},{"cell_type":"code","source":"import pypandoc\n\ndef convert_rtf_to_text(file_path):\n    output = pypandoc.convert_file(json_file_path, 'plain', format='rtf')\n    return output\n\n# Example usage\njson_file_path = '/kaggle/input/jason-parsing/algoparams_from_ui.json.rtf'\ntext = convert_rtf_to_text(json_file_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:18:34.360099Z","iopub.execute_input":"2024-06-14T09:18:34.361410Z","iopub.status.idle":"2024-06-14T09:18:35.349142Z","shell.execute_reply.started":"2024-06-14T09:18:34.361363Z","shell.execute_reply":"2024-06-14T09:18:35.347888Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"stripped_text = text.replace('\\n', '')\ntext = stripped_text.replace(' ', '')\nconstraint_dict = json.loads(text)\nprint(type(constraint_dict))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:18:40.937909Z","iopub.execute_input":"2024-06-14T09:18:40.938365Z","iopub.status.idle":"2024-06-14T09:18:40.946425Z","shell.execute_reply.started":"2024-06-14T09:18:40.938319Z","shell.execute_reply":"2024-06-14T09:18:40.944960Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<class 'dict'>\n","output_type":"stream"}]},{"cell_type":"code","source":"print(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_with'])","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:01:01.755476Z","iopub.execute_input":"2024-06-14T07:01:01.756399Z","iopub.status.idle":"2024-06-14T07:01:01.763536Z","shell.execute_reply.started":"2024-06-14T07:01:01.756357Z","shell.execute_reply":"2024-06-14T07:01:01.762164Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"custom\n","output_type":"stream"}]},{"cell_type":"code","source":"class DataLoader:\n    def __init__(self, data_file_path):\n        self.data_file_path = data_file_path\n        self.data = None\n        self.X_train, self.X_test, self.y_train, self.y_test = None, None, None, None\n\n    def load_data(self):\n        \"\"\" Loading the dataset \"\"\" \n        self.data = pd.read_csv(self.data_file_path)\n        \n        # Split the dataset into features and target variable\n        X = self.data.drop(columns='species') \n        y = self.data['species']\n        \n        encoder = LabelEncoder()\n        y = encoder.fit_transform(y)\n        \n        \n        # Split the dataset into training and testing sets\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n        print(\"Data loaded successfully\")\n    \n    def feature_handling(self):\n        print(\"Checking\")\n        \"\"\" Delete columns if is_selected is false \"\"\"\n        if(constraint_dict['design_state_data']['feature_handling']['sepal_length']['is_selected'] == False):\n            print(\"sepal_length False\")\n            self.X_train = self.X_train.drop('sepal_length', axis = 1)\n            self.X_test = self.X_test.drop('sepal_length', axis = 1)\n        \n        if(constraint_dict['design_state_data']['feature_handling']['sepal_width']['is_selected'] == False):\n            print(\"False\")\n            self.X_train = self.X_train.drop('sepal_width', axis = 1)\n            self.X_test = self.X_test.drop('sepal_width', axis = 1)\n            \n        if(constraint_dict['design_state_data']['feature_handling']['petal_length']['is_selected'] == False):\n            print(\"False\")\n            self.X_train = self.X_train.drop('petal_length', axis = 1)\n            self.X_test = self.X_test.drop('petal_length', axis = 1)\n            \n        if(constraint_dict['design_state_data']['feature_handling']['petal_width']['is_selected'] == False):\n            print(\"False\")\n            self.X_train = self.X_train.drop('petal_width', axis = 1)\n            self.X_test = self.X_test.drop('petal_width', axis = 1)\n    \n    def impute_operations(self):\n        \"\"\" Impute with mean or custom value. This works only for mean and custom values\"\"\"\n        print(\"Imputing\")\n        if(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_with'] == 'Averageofvalues'):\n            imputer = SimpleImputer(strategy=\"mean\")\n            self.X_train['sepal_length'] = imputer.fit_transform(self.X_train[['sepal_length']])\n        elif(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_with'] == 'custom'):\n            imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['sepal_length']['feature_details']['impute_value']))\n            self.X_train['sepal_length'] = imputer.fit_transform(self.X_train[['sepal_length']])\n            \n            \n        if(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_with'] == 'Averageofvalues'):\n            imputer = SimpleImputer(strategy=\"mean\")\n            self.X_train['sepal_width'] = imputer.fit_transform(self.X_train[['sepal_width']])\n        elif(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_with'] == 'custom'):\n            imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['sepal_width']['feature_details']['impute_value']))\n            self.X_train['sepal_width'] = imputer.fit_transform(self.X_train[['sepal_width']])\n            \n        \n        if(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_with'] == 'Averageofvalues'):\n            imputer = SimpleImputer(strategy=\"mean\")\n            self.X_train['petal_length'] = imputer.fit_transform(self.X_train[['petal_length']])\n        elif(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_with'] == 'custom'):\n            imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['petal_length']['feature_details']['impute_value']))\n            self.X_train['petal_length'] = imputer.fit_transform(self.X_train[['petal_length']])\n            \n            \n        if(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_with'] == 'Averageofvalues'):\n            imputer = SimpleImputer(strategy=\"mean\")\n            self.X_train['petal_width'] = imputer.fit_transform(self.X_train[['petal_width']])\n        elif(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_with'] == 'custom'):\n            imputer = SimpleImputer(strategy=\"constant\", fill_value=int(constraint_dict['design_state_data']['feature_handling']['petal_width']['feature_details']['impute_value']))\n            self.X_train['petal_width'] = imputer.fit_transform(self.X_train[['petal_width']])\n            \n            \n        \n        \n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:20:03.818420Z","iopub.execute_input":"2024-06-14T09:20:03.818851Z","iopub.status.idle":"2024-06-14T09:20:03.846518Z","shell.execute_reply.started":"2024-06-14T09:20:03.818817Z","shell.execute_reply":"2024-06-14T09:20:03.845254Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"data = DataLoader('/kaggle/input/iris-dataset-dendrite/iris.csv')\ndata.load_data()\ndata.feature_handling()\ndata.impute_operations()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T09:19:30.297991Z","iopub.execute_input":"2024-06-14T09:19:30.298421Z","iopub.status.idle":"2024-06-14T09:19:30.335211Z","shell.execute_reply.started":"2024-06-14T09:19:30.298389Z","shell.execute_reply":"2024-06-14T09:19:30.333983Z"},"trusted":true},"execution_count":41,"outputs":[{"name":"stdout","text":"Data loaded successfully\nChecking\nImputing\n     sepal_length  sepal_width  petal_length  petal_width\n22            4.6          3.6           1.0          0.2\n15            5.7          4.4           1.5          0.4\n65            6.7          3.1           4.4          1.4\n11            4.8          3.4           1.6          0.2\n42            4.4          3.2           1.3          0.2\n..            ...          ...           ...          ...\n71            6.1          2.8           4.0          1.3\n106           4.9          2.5           4.5          1.7\n14            5.8          4.0           1.2          0.2\n92            5.8          2.6           4.0          1.2\n102           7.1          3.0           5.9          2.1\n\n[120 rows x 4 columns]\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"class PipelineData(DataLoader):\n    def __init__(self, data_file_path):\n        super().__init__(data_file_path)\n        self.pipeline_data = Pipeline([])\n        print(\"Data Pipeline Created\")\n        \n    \n            \n        \n        ","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:38:32.336051Z","iopub.execute_input":"2024-06-13T10:38:32.336723Z","iopub.status.idle":"2024-06-13T10:38:32.360619Z","shell.execute_reply.started":"2024-06-13T10:38:32.336681Z","shell.execute_reply":"2024-06-13T10:38:32.357654Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"# pipelineData = PipelineData('/kaggle/input/iris-dataset-dendrite/iris.csv')\n# pipelineData.load_data()","metadata":{"execution":{"iopub.status.busy":"2024-06-13T08:49:52.878766Z","iopub.execute_input":"2024-06-13T08:49:52.879524Z","iopub.status.idle":"2024-06-13T08:49:52.899205Z","shell.execute_reply.started":"2024-06-13T08:49:52.879460Z","shell.execute_reply":"2024-06-13T08:49:52.897457Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Data loaded successfully\n","output_type":"stream"}]},{"cell_type":"code","source":"class PipelineModel(PipelineData):\n    def __init__(self, data_file_path):\n        super().__init__(data_file_path)\n    \n    \n    def load_feature_handling():\n        super().load_data()\n        self.pipeline_model = Pipeline([])\n        print(\"Data and Model Pipeline created successfully\")\n#         super().feature_handling()\n        \n    \n    \n        \n","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:38:34.753276Z","iopub.execute_input":"2024-06-13T10:38:34.755053Z","iopub.status.idle":"2024-06-13T10:38:34.763299Z","shell.execute_reply.started":"2024-06-13T10:38:34.755000Z","shell.execute_reply":"2024-06-13T10:38:34.761604Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"\nmodel_pipe = PipelineModel('/kaggle/input/iris-dataset-dendrite/iris.csv')\nmodel_pipe.X_train","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:39:34.630708Z","iopub.execute_input":"2024-06-13T10:39:34.631265Z","iopub.status.idle":"2024-06-13T10:39:34.639468Z","shell.execute_reply.started":"2024-06-13T10:39:34.631229Z","shell.execute_reply":"2024-06-13T10:39:34.637759Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Data Pipeline Created\n","output_type":"stream"}]},{"cell_type":"code","source":"print(model_pipe.X_train)","metadata":{"execution":{"iopub.status.busy":"2024-06-13T10:39:38.078856Z","iopub.execute_input":"2024-06-13T10:39:38.079410Z","iopub.status.idle":"2024-06-13T10:39:38.087276Z","shell.execute_reply.started":"2024-06-13T10:39:38.079347Z","shell.execute_reply":"2024-06-13T10:39:38.085536Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"None\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values=np.nan, strategy='mean')\nprint(X)\nX = [[np.nan, 2], [6, np.nan], [7, 6]]\nimp.fit(X)\nprint(imp.transform(X))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:07:41.711371Z","iopub.execute_input":"2024-06-14T06:07:41.711791Z","iopub.status.idle":"2024-06-14T06:07:41.721473Z","shell.execute_reply.started":"2024-06-14T06:07:41.711759Z","shell.execute_reply":"2024-06-14T06:07:41.720052Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"[[nan, 2], [6, nan], [7, 6]]\n[[6.5 2. ]\n [6.  4. ]\n [7.  6. ]]\n","output_type":"stream"}]},{"cell_type":"code","source":"import scipy.sparse as sp\nX = np.array([[1, 0], [0, 8], [8, 4]])\nprint(X[:,1])\n# imp = SimpleImputer(missing_values=0, strategy='mean')\n# imp.fit(X[:,1])\n# print(imp.transform(X[:,1]))","metadata":{"execution":{"iopub.status.busy":"2024-06-14T06:26:13.323969Z","iopub.execute_input":"2024-06-14T06:26:13.324990Z","iopub.status.idle":"2024-06-14T06:26:13.332749Z","shell.execute_reply.started":"2024-06-14T06:26:13.324947Z","shell.execute_reply":"2024-06-14T06:26:13.331208Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"[0 8 4]\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.datasets import load_iris\nfrom nltk.tokenize import word_tokenize  # for text tokenization\nfrom sklearn.feature_extraction.text import HashingVectorizer  # for hashing\n\n# Load the iris dataset\niris = load_iris()\n\n# Check the data type of 'species' (should be object for text data)\nprint(iris.target_names)  # Assuming 'species' is stored in target_names\n\n# No need for separate tokenization as target_names are already single words\nspecies_tokens = iris.target_names  # Assuming each species is a single word\n\n# Hashing with 16 features\nhasher = HashingVectorizer(n_features=3)\n\n# Transform tokens (assuming each species is a single word)\nhashed_species = hasher.transform(species_tokens)\n\n# Print the hashed features (dense matrix)\nprint(hashed_species.toarray())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T07:24:25.070761Z","iopub.execute_input":"2024-06-14T07:24:25.071200Z","iopub.status.idle":"2024-06-14T07:24:25.091006Z","shell.execute_reply.started":"2024-06-14T07:24:25.071166Z","shell.execute_reply":"2024-06-14T07:24:25.089383Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stdout","text":"['setosa' 'versicolor' 'virginica']\n[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport pandas as pd\nfrom sklearn.feature_extraction import FeatureHasher\n\n# Load the Iris dataset\nfile_path = '/kaggle/input/iris-dataset-dendrite/iris.csv'  # Update with the correct path to your Iris dataset\niris = pd.read_csv(file_path)\n\n# Ensure the 'species' column is treated as a string\niris['species'] = iris['species'].astype(str)\n\n# Prepare the species column as an iterable of iterables\nspecies_iterable = [[species] for species in iris['species']]\n\n# Define the FeatureHasher\nhasher = FeatureHasher(input_type='string', n_features=0)  # Adjust n_features as needed\n\n# Apply the hasher to the 'species' column\nhashed_features = hasher.transform(species_iterable)\n\n# Convert the hashed features to a DataFrame\nhashed_df = pd.DataFrame(hashed_features.toarray(), columns=[f'hashed_feature_{i}' for i in range(hashed_features.shape[1])])\n\n# Drop the original 'species' column and concatenate the hashed features\niris_transformed = pd.concat([iris.drop(columns=['species']), hashed_df], axis=1)\n\n# Display the transformed dataset\nprint(iris_transformed.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T18:00:40.657546Z","iopub.execute_input":"2024-06-14T18:00:40.657929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Load the Iris dataset\nfile_path = '/kaggle/input/iris-dataset-dendrite/iris.csv'  # Update with the correct path to your Iris dataset\niris = pd.read_csv(file_path)\n\n# Ensure 'species' is the target column and drop it from features\nfeatures = iris.drop(columns=['species'])\ntarget = iris['species']\n\n# Generate specific linear interaction feature: petal_length * sepal_width\nfeatures['petal_length_sepal_width'] = features['petal_length'] * features['sepal_width']\n\n# Concatenate the interaction features with the original target column\niris_with_interactions = pd.concat([features, target], axis=1)\n\n# Display the transformed dataset\nprint(iris_with_interactions)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T17:59:48.919672Z","iopub.execute_input":"2024-06-14T17:59:48.920698Z","iopub.status.idle":"2024-06-14T17:59:49.376939Z","shell.execute_reply.started":"2024-06-14T17:59:48.920650Z","shell.execute_reply":"2024-06-14T17:59:49.375811Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"     sepal_length  sepal_width  petal_length  petal_width  \\\n0             5.1          3.5           1.4          0.2   \n1             4.9          3.0           1.4          0.2   \n2             4.7          3.2           1.3          0.2   \n3             4.6          3.1           1.5          0.2   \n4             5.0          3.6           1.4          0.2   \n..            ...          ...           ...          ...   \n145           6.7          3.0           5.2          2.3   \n146           6.3          2.5           5.0          1.9   \n147           6.5          3.0           5.2          2.0   \n148           6.2          3.4           5.4          2.3   \n149           5.9          3.0           5.1          1.8   \n\n     petal_length_sepal_width         species  \n0                        4.90     Iris-setosa  \n1                        4.20     Iris-setosa  \n2                        4.16     Iris-setosa  \n3                        4.65     Iris-setosa  \n4                        5.04     Iris-setosa  \n..                        ...             ...  \n145                     15.60  Iris-virginica  \n146                     12.50  Iris-virginica  \n147                     15.60  Iris-virginica  \n148                     18.36  Iris-virginica  \n149                     15.30  Iris-virginica  \n\n[150 rows x 6 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\n\n# Load the Iris dataset\nfile_path = '/kaggle/input/iris-dataset-dendrite/iris.csv'  # Update with the correct path to your Iris dataset\niris = pd.read_csv(file_path)\n\n# Split the dataset into features and target variable\nX = iris.drop(columns='species')  # Assuming 'species' is the target column\ny = iris['species']\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize a Random Forest classifier\nrf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nrf.fit(X_train, y_train)\n\n# Use SelectFromModel to perform feature selection based on feature importance\nselector = SelectFromModel(rf, threshold=\"mean\", prefit=True)\nX_train_reduced = selector.transform(X_train)\nX_test_reduced = selector.transform(X_test)\n\n# Get the selected feature names\nselected_features = X.columns[selector.get_support()]\n\n# Create a DataFrame with the selected features for train and test sets\nX_train_reduced_df = pd.DataFrame(X_train_reduced, columns=selected_features)\nX_test_reduced_df = pd.DataFrame(X_test_reduced, columns=selected_features)\n\n# Display the reduced feature set\nprint(\"Selected features:\")\nprint(selected_features)\nprint(\"\\nTransformed training data:\")\nprint(X_train_reduced_df.head())\nprint(\"\\nTransformed testing data:\")\nprint(X_test_reduced_df.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-06-14T18:07:00.483188Z","iopub.execute_input":"2024-06-14T18:07:00.483588Z","iopub.status.idle":"2024-06-14T18:07:00.769926Z","shell.execute_reply.started":"2024-06-14T18:07:00.483554Z","shell.execute_reply":"2024-06-14T18:07:00.768959Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Selected features:\nIndex(['petal_length', 'petal_width'], dtype='object')\n\nTransformed training data:\n   petal_length  petal_width\n0           1.0          0.2\n1           1.5          0.4\n2           4.4          1.4\n3           1.6          0.2\n4           1.3          0.2\n\nTransformed testing data:\n   petal_length  petal_width\n0           4.7          1.2\n1           1.7          0.3\n2           6.9          2.3\n3           4.5          1.5\n4           4.8          1.4\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/base.py:432: UserWarning: X has feature names, but SelectFromModel was fitted without feature names\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}